{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointer Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import local files\n",
    "import vocabulary as vocab\n",
    "from example import Example\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocabulary import UNK\n",
    "from vocabulary import START\n",
    "from vocabulary import END\n",
    "from vocabulary import PAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_excel('Dataset.xlsx', 'Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into input out sentences\n",
    "src = np.array(dataset['Src'])\n",
    "target = np.array(dataset['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function being using to create vocabulary from src sentences\n",
    "def preprocess_src(text):\n",
    "    text = str(text)\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "for i in range(0, len(src)):\n",
    "    src[i] = preprocess_src(src[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = vocab.Vocabulary()\n",
    "target_vocab = vocab.Vocabulary()"
   ]
  },
  {
   "source": [
    "Run either the first cell or the second cell below this cell to load the vocabulary. Do not run them both.\n",
    "\n",
    "While training of this model, the vocabulary was generated from the training set (second cell)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary using pretrained glove word embedding. Glove word embedding of any dimension can be loaded through this function.\n",
    "# Use the last parameter load_word_embedding=False to load only the vocabulary from the word embedding file\n",
    "# and not the word embeddings\n",
    "vocab.load_from_glove_vector(src_vocab, 'glove.6B.100d.txt', load_word_embedding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate vocabulary using sentences. The structure sentences should be a list of lists\n",
    "# [ [sentence 1], [sentence 2], [sentence 3]... ]\n",
    "vocab.generate_vocab_from_text(src_vocab, src, 5000)"
   ]
  },
  {
   "source": [
    "Load python vocabulary, the vocabulary of output sentences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads vocabulary from excel file. First argument is the file name, second argument is the sheet name.\n",
    "vocab.load_vocab_from_excel(\n",
    "    target_vocab, 'Python Vocabulary.xlsx', 'Python Vocabulary.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max length of source and target sentences\n",
    "src_max_len = max([len(str(sentence).split()) for sentence in src])\n",
    "target_max_len = max([len(str(sentence).split()) for sentence in target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "examples = []\n",
    "for i, j in zip(src, target):\n",
    "    examples.append(\n",
    "        Example(i, j, src_vocab, target_vocab, src_max_len, target_max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle examples\n",
    "np.random.shuffle(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert examples (Only for Attention mechanism)"
   ]
  },
  {
   "source": [
    "Convert the examples so that they are compatible to be used with Attention mechanism. Attention mechanism will allow the <unk> tokens to be replaced with the words in the input sentence. If you want this to be a simple Seq2Seq model without attention, do not run the cell below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(examples)):\n",
    "    for j in range(0, len(examples[i].enc_input)):\n",
    "        if examples[i].enc_input[j] >= src_vocab.vocab_size:\n",
    "            examples[i].enc_input[j] = src_vocab.word_index[UNK]\n",
    "\n",
    "for i in range(0, len(examples)):\n",
    "    for j in range(0, len(examples[i].dec_input)):\n",
    "        if examples[i].dec_input[j] >= target_vocab.vocab_size:\n",
    "            examples[i].dec_input[j] = target_vocab.word_index[UNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "test_size = 0 # in %age\n",
    "# Get test_size % of examples\n",
    "num_of_train_exs = len(examples) - int((test_size/100) * len(examples))\n",
    "x_train = examples[:num_of_train_exs]\n",
    "x_test = examples[num_of_train_exs:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of training examples:', len(x_train))\n",
    "print('Number of testing examples:', len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = src_vocab.vocab_size\n",
    "target_vocab_size = target_vocab.vocab_size\n",
    "# Get the max number of source oov word for extended vocabulary\n",
    "max_src_oov_word = max([len(ex.source_oov_words) for ex in examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(examples)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(examples)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, enc_units, emb_dims, src_vocab_size):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Define lstm\n",
    "        self.lstm1 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(\n",
    "                enc_units, return_sequences=True, recurrent_initializer='glorot_uniform'))\n",
    "        self.lstm2 = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(\n",
    "                enc_units, return_sequences=True, recurrent_initializer='glorot_uniform'))\n",
    "                \n",
    "        self.src_embedding = tf.keras.layers.Embedding(src_vocab_size + 1, emb_dims)\n",
    "    \n",
    "    def call(self, x):\n",
    "        # x: (batch_size, max_inp_len) np.ndarray\n",
    "\n",
    "        x = tf.convert_to_tensor(x)\n",
    "        \n",
    "        embeddings = self.src_embedding(x) # (batch size, max_inp_len, emb_dims)\n",
    "        \n",
    "        # Pass through the LSTM cell\n",
    "        activations = self.lstm1(embeddings) # (batch size, max_inp_len, enc_units)\n",
    "        activations = self.lstm2(activations) # (batch size, max_inp_len, enc_units)\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, activations, s_prev):\n",
    "        # activations (batch size, max_inp_length, enc_units)\n",
    "        # s_prev (batch size, dec_units)\n",
    "\n",
    "        # Add a dimension to s_prev\n",
    "        s_prev = tf.expand_dims(s_prev, 1) # (batch size, 1, dec_units)\n",
    "        w1 = self.W1(activations) # (batch size, max_inp_length, units)\n",
    "        w2 = self.W2(s_prev) # (batch size, 1, units)\n",
    "        self.score = self.V(tf.nn.tanh(w1 + w2)) # (batch size, max_inp_length, units)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(self.score, axis=1) # (batch size, max_inp_length, 1)\n",
    "        context = attention_weights * activations # (batch size, max_inp_length, enc_units)\n",
    "        context = tf.reduce_sum(context, axis=1) # (batch size, enc_units)\n",
    "\n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGen(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(PGen, self).__init__()\n",
    "        self.Wh = tf.keras.layers.Dense(units)\n",
    "        self.Ws = tf.keras.layers.Dense(units)\n",
    "        self.Wx = tf.keras.layers.Dense(units)\n",
    "        self.pgen = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, x, dec_hidden, context_vec):\n",
    "        # x: word embedding at time t (batch size, emb_dims)\n",
    "        # dec_hidden: hidden state of decoder at time t (batch size, dec_units)\n",
    "        # context_vec: context vector at time t (batch size, enc_units)\n",
    "\n",
    "        wx = self.Wx(x) # (batch size, units)\n",
    "        ws = self.Ws(dec_hidden) # (batch size, units)\n",
    "        wh = self.Wh(context_vec) # (batch size, units)\n",
    "\n",
    "        pgen = self.pgen(wx + ws + wh) # (batch size, 1)\n",
    "\n",
    "        return pgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, dec_units, emb_dims, target_vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "\n",
    "        self.target_embedding = tf.keras.layers.Embedding(target_vocab_size + 1, emb_dims)\n",
    "        # Define LSTM cell\n",
    "        self.lstm = tf.keras.layers.LSTM(dec_units, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "        self.attention = Attention(dec_units)\n",
    "\n",
    "        self.pgen = PGen(dec_units)\n",
    "\n",
    "    def call(self, x, activations, dec_hidden, dec_cell):\n",
    "        # x: (batch size, 1)\n",
    "        # activations: (batch size, max_inp_length, enc_units)\n",
    "        # dec_hidden: (batch size, dec_units)\n",
    "        # dec_cell: (batch size, dec_units)\n",
    "\n",
    "        # Get context vector (batch size, enc_units) and attention weights (batch size, max_inp_length, 1)\n",
    "        context, attention_weights = self.attention(activations, dec_hidden)\n",
    "\n",
    "        # Get embeddings\n",
    "        x = self.target_embedding(x) # (batch size, 1, emb_dims)\n",
    "\n",
    "        # Calculate pgen\n",
    "        pgen = self.pgen(tf.reshape(x, [x.shape[0], -1]), dec_hidden, context)\n",
    "\n",
    "        # concatenate word embeddings and context vector\n",
    "        # concatenate on last axis\n",
    "        concat = tf.concat([tf.expand_dims(context, 1), x], axis=-1) # (batch size, 1, emb_dims + enc_units)\n",
    "\n",
    "        # Pass through the lstm and get hidden state and cell state\n",
    "        dec_hidden, _, dec_cell = self.lstm(concat, initial_state=[dec_hidden, dec_cell])\n",
    "\n",
    "        # Get the prediction at time t\n",
    "        predictions = self.fc(dec_hidden) # (batch size, target_vocab_size)\n",
    "\n",
    "        # Softmax the predictions\n",
    "        predictions = tf.nn.softmax(predictions, axis=1)\n",
    "\n",
    "        return dec_hidden, dec_cell, predictions, tf.reshape(attention_weights, [x.shape[0], -1]), pgen\n",
    "\n",
    "    def initialize_states(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.dec_units)), tf.zeros((batch_size, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoder and decoder\n",
    "encoder = Encoder(units, embedding_dim, src_vocab_size)\n",
    "decoder = Decoder(units, embedding_dim, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if you have saved weights\n",
    "encoder.load_weights('enc_weights')\n",
    "decoder.load_weights('dec_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the attention over non-pad tokens\n",
    "# enc_pad_mask is a list of lists of 1s and 0s. The 0s represent the padded area of encoder input\n",
    "def apply_attention_mask(attention_dists, enc_pad_mask):\n",
    "    # attention_dist: (batch size, src_max_len)\n",
    "    # enc_pad_mask: (batch size, src_max_len)\n",
    "    attention_dists = tf.math.multiply(attention_dists, enc_pad_mask)\n",
    "    \n",
    "    masked_sum = tf.reduce_sum(attention_dists, axis=-1) # (batch size,)\n",
    "                            # (batch size, 1)\n",
    "    return attention_dists / tf.reshape(masked_sum, [-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final distribution using vocabulary distribution and copy distribution\n",
    "def calculate_final_dist(inp, vocab_dists, copy_dists, enc_pad_mask, pgen, max_src_oov_word, ptr_net=False):\n",
    "    # inp : (batch size, src_max_len)\n",
    "    # vocab_dist, the predictions of one time step for the whole batch (batch size, target_vocab_size)\n",
    "    # The attention for one time step for over the whole batch (batch size, src_max_len)\n",
    "    if ptr_net:\n",
    "        copy_dists = apply_attention_mask(copy_dists, enc_pad_mask)\n",
    "\n",
    "        vocab_dists = tf.math.multiply(pgen, vocab_dists)\n",
    "        copy_dists = tf.math.multiply((1 - pgen), copy_dists)\n",
    "        # (batch size, src_vocab_size + max_src_oov_word)\n",
    "        copy_dists_projected = []\n",
    "\n",
    "        for i in range(inp.shape[0]):\n",
    "            copy_dists_projected.append(\n",
    "                tf.scatter_nd(\n",
    "                    tf.expand_dims(inp[i, :], 1), copy_dists[i, :], [src_vocab_size + max_src_oov_word]))\n",
    "        # Concatenate vocab_dist and copy dist\n",
    "        # (batch size, target_vocab_size + src_vocab_size + max_src_oov_word)\n",
    "        final_dists = tf.concat([vocab_dists, copy_dists_projected], axis=1)\n",
    "\n",
    "        return final_dists\n",
    "    else:\n",
    "        # else final distribution is just the vocabulary distribution (Seq2Seq with Attention)\n",
    "        # (batch size, target_vocab_size)\n",
    "        return vocab_dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "def loss_function(real, pred, dec_pad_mask):\n",
    "    # real = target (batch size, 1)\n",
    "    # (batch size, target_vocab_size + src_vocab_size + max_src_oov_word) or\n",
    "    # (batch size, target_vocab_size)\n",
    "    # pred = final_dists, we'll get from calculate_final_dist function\n",
    "    # ded_pad_mask: (batch size, 1)\n",
    "\n",
    "    # Get probability of correct labels\n",
    "    batch_sz = tf.expand_dims(tf.range(0, limit=real.shape[0]), 1) # (batch size, 1)\n",
    "    real = tf.stack((batch_sz, real), 2)\n",
    "    correct_probs = tf.gather_nd(pred, real) # (batch size, 1)\n",
    "    # Calculate negtaive log likelihood of corrent probs\n",
    "    loss = -tf.math.log(correct_probs) # (batch size, 1)\n",
    "    # Apply dec_pad_mask to exclude the loss associated with PAD tokens\n",
    "    loss = tf.math.multiply(loss, tf.cast(dec_pad_mask, dtype=tf.float32)) # (batch size, )\n",
    "    # Take the average loss of the whole batch\n",
    "    mean_loss = tf.reduce_mean(loss) # (1, 1)\n",
    "\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inp, targ, enc_pad_mask, dec_pad_mask):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get encoder output\n",
    "        activations = encoder(inp)\n",
    "\n",
    "        # Initialize decoder states\n",
    "        dec_hidden, dec_cell = decoder.initialize_states(targ.shape[0])\n",
    "\n",
    "        # Initial token (START)\n",
    "        dec_inp = tf.expand_dims([target_vocab.word_index[START]] * targ.shape[0], 1)\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # Get decoder output\n",
    "            dec_hidden, dec_cell, predictions, attention_weights, pgen = \\\n",
    "                decoder(dec_inp, activations, dec_hidden, dec_cell)\n",
    "\n",
    "            # Calculate final distribution (vocabulary distribution + attention over inputs)\n",
    "            # Set the last argument ptr_net=False if you want to disable the attention mechanism,\n",
    "            # in that case the final distribution will be the vocabulary distrbution only\n",
    "            final_dists = calculate_final_dist(\n",
    "                inp, predictions, attention_weights, enc_pad_mask, pgen, max_src_oov_word, ptr_net=True)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss += \\\n",
    "                loss_function(tf.expand_dims(targ[:, t], 1), final_dists, tf.expand_dims(dec_pad_mask[:, t], 1))\n",
    "\n",
    "            for i in range(0, targ.shape[0]):\n",
    "                if targ[i, t] >= target_vocab_size:\n",
    "                    targ[i, t] = target_vocab.word_index[UNK]\n",
    "            # Get next input for decoder\n",
    "            dec_inp = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "        # Calculate loss over the whole batch\n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        # Get trainable variables\n",
    "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        \n",
    "        # Compute gradient w.r.t loss\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        \n",
    "        # Back propagate and update weights\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "        \n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    np.random.shuffle(x_train)\n",
    "\n",
    "    for batch, i in enumerate(range(0, len(x_train), BATCH_SIZE)):\n",
    "        batch_length = i + 64\n",
    "        if batch_length > len(x_train):\n",
    "            exs = x_train[i:len(x_train)]\n",
    "        else:\n",
    "            exs = x_train[i:batch_length]\n",
    "\n",
    "        inp = []\n",
    "        targ = []\n",
    "        enc_pad_mask = []\n",
    "        dec_pad_mask = []\n",
    "\n",
    "        for ex in exs:\n",
    "            inp.append(ex.enc_input)\n",
    "            targ.append(ex.dec_input)\n",
    "            enc_pad_mask.append(ex.enc_pad_mask)\n",
    "            dec_pad_mask.append(ex.dec_pad_mask)\n",
    "\n",
    "        inp = np.array(inp)\n",
    "        targ = np.array(targ)\n",
    "        enc_pad_mask = np.array(enc_pad_mask)\n",
    "        dec_pad_mask = np.array(dec_pad_mask)\n",
    "\n",
    "        batch_loss = train_step(inp[:], targ[:], enc_pad_mask[:], dec_pad_mask[:])\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 5 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                        batch,\n",
    "                                                        batch_loss.numpy()))\n",
    "    # Store loss per epoch\n",
    "    loss_plot.append(total_loss / steps_per_epoch)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "encoder.save_weights('enc_weights', save_format='tf')\n",
    "decoder.save_weights('dec_weights', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.plot(loss_plot)\n",
    "plt.xlabel('EPOCHS')\n",
    "plt.ylabel('LOSS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp):\n",
    "    # Generate example\n",
    "    ex = Example(inp, '', src_vocab, target_vocab, src_max_len, 0)\n",
    "    # Pass the input to the encoder\n",
    "    inp = np.reshape(ex.enc_input, (1, len(ex.enc_input)))\n",
    "\n",
    "    activations = encoder(inp)\n",
    "\n",
    "    dec_inp = tf.expand_dims([ex.dec_input], 0)\n",
    "    dec_hidden, dec_cell = decoder.initialize_states(1)\n",
    "\n",
    "    counter = 0\n",
    "    output = '<start> '\n",
    "\n",
    "    while counter < target_max_len:\n",
    "        dec_hidden, dec_cell, predictions, attention_weights, pgen = decoder(\n",
    "                dec_inp, activations, dec_hidden, dec_cell)\n",
    "        \n",
    "        final_dist = calculate_final_dist(\n",
    "            inp, predictions, attention_weights, ex.enc_pad_mask, pgen, len(ex.source_oov_words), ptr_net=True)\n",
    "\n",
    "        prediction_idx = tf.argmax(final_dist[0]).numpy()\n",
    "\n",
    "        if prediction_idx < target_vocab_size:\n",
    "            output += target_vocab.index_word[prediction_idx] + ' '\n",
    "\n",
    "            if target_vocab.index_word[prediction_idx] == END:\n",
    "                return output.strip(), ex.preprocessed_enc_input\n",
    "\n",
    "        elif prediction_idx >= target_vocab_size and prediction_idx < (src_vocab_size + target_vocab_size):\n",
    "            output += src_vocab.index_word[prediction_idx - target_vocab_size] + ' '\n",
    "\n",
    "            if src_vocab.index_word[prediction_idx - target_vocab_size] == END:\n",
    "                return output.strip(), ex.preprocessed_enc_input\n",
    "\n",
    "        else:\n",
    "            output += ex.source_oov_words[prediction_idx - src_vocab_size - target_vocab_size]\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    output += '<end>'\n",
    "    return output.strip(), ex.preprocessed_enc_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    result, sent = evaluate(examples[i].preprocessed_enc_input)\n",
    "    print(result)\n",
    "    print(sent, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "0bf36e21-5cc3-4daa-9a41-197de28045e8",
   "display_name": "'Python Interactive'"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}